{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_NeNEs3VhfF"
      },
      "source": [
        "## AMAZON WEB SCRAPING USING PYTHON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWzr6MYMba4h"
      },
      "source": [
        "![image](https://miro.medium.com/max/1400/1*kr-TlPT8c7_ZR4iC9QILWg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSOJDaqXbcf"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1EjOKpgXnMR"
      },
      "source": [
        "### 1.1 Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerGe5UXVtwK"
      },
      "source": [
        "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
        "\n",
        "In thisproject we will extract attributes ofthe best seller books.we will then collect a single product's attributes and create an email alert for when prices\n",
        "of the item goes down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5RBE_KiXuX6"
      },
      "source": [
        "### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFeODDyOWYWv"
      },
      "source": [
        "In order to concentrate on competitor price research, real-time cost monitoring and seasonal shifts in order to provide consumers with better product offers. Web scraping Amazon data is needed for you to extract relevant data from the Amazon website and save it in a spreadsheet or JSON format. You can even automate the process to update the data on a regular weekly or monthly basis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp1OENwEZnpb"
      },
      "source": [
        "## 2. Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "e2h6cOZIUSwT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuaS9usUZzjU"
      },
      "source": [
        "## 3. Defining base URL and data pulling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNNsL9bMSBn9"
      },
      "source": [
        "we will be extracting the best seller books attributes from this  url https://www.amazon.in/gp/bestsellers/books/\n",
        "and the data to be collected is\n",
        "\n",
        "\n",
        "*   Book Name\n",
        "*   Author\n",
        "* Rating\n",
        "* Customers Rated\n",
        "* Price\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3pnCZKi-qj8"
      },
      "source": [
        " Python contains an amazing library called BeautifulSoup to allow web scraping. We will be using it to scrape product information and save the details in a CSV file.\n",
        " in the following functions we will declare a Header and add a user agent. This ensures that the target website we are going to web scrape doesn’t consider traffic from our program as spam and finally get blocked by them.\n",
        "To pinpoint our target elements, we will grab its element classes and feed them to the script. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "XbjnXTp2T3EF"
      },
      "outputs": [],
      "source": [
        "no_pages = 2\n",
        "\n",
        "def get_data(pageNo):  \n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; https://www.amazon.in/gp/bestsellers/books/ref=zg_bs_pg_x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    r = requests.get('https://www.amazon.in/gp/bestsellers/books/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo), headers=headers)#, proxies=proxies)\n",
        "    content = r.content\n",
        "    soup1 = BeautifulSoup(content,\"html.parser\")\n",
        "    soup = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
        "    #print(soup)\n",
        "\n",
        "    alls = []\n",
        "    for d in soup.findAll('div', attrs={'class':'a-cardui _cDEzb_grid-cell_1uMOS p13n-grid-content'}):\n",
        "        #print(d)\n",
        "        name = d.find('span', attrs={'class':'_cDEzb_p13n-sc-css-line-clamp-1_1Fn1y'})\n",
        "        n = name.find_all('img', alt=True)\n",
        "        #print(n[0]['alt'])\n",
        "        author = d.find('div', attrs={'class':'_cDEzb_p13n-sc-css-line-clamp-1_1Fn1y'})\n",
        "        print(author)\n",
        "        rating = d.find('a', attrs={'class':'a-link-normal'})\n",
        "        users_rated = d.find('span', attrs={'class':'a-size-small'})\n",
        "        <span class=\"a-offscreen\">₹4,049.00</span>\n",
        "\n",
        "        all1=[]\n",
        "\n",
        "        if name is not None:\n",
        "            #print(n[0]['alt'])\n",
        "            all1.append(n[0]['alt'])\n",
        "        else:\n",
        "            all1.append(\"unknown-product\")\n",
        "\n",
        "        if author is not None:\n",
        "            #print(author.text)\n",
        "            all1.append(author.text)\n",
        "        elif author is None:\n",
        "            author = d.find('span', attrs={'class':'a-size-small a-color-base'})\n",
        "            if author is not None:\n",
        "                all1.append(author.text)\n",
        "            else:    \n",
        "                all1.append('0')\n",
        "\n",
        "        if rating is not None:\n",
        "            #print(rating.text)\n",
        "            all1.append(rating.text)\n",
        "        else:\n",
        "            all1.append('-1')\n",
        "\n",
        "        if users_rated is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(users_rated.text)\n",
        "        else:\n",
        "            all1.append('0')     \n",
        "\n",
        "        if price is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(price.text)\n",
        "        else:\n",
        "            all1.append('0')\n",
        "        alls.append(all1)    \n",
        "    return alls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "86I7cOh5ZA3G"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for i in range(1, no_pages+1):\n",
        "  results.append(get_data(i))\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "df = pd.DataFrame(flatten(results),columns=['Book Name','Author','Rating','Customers_Rated', 'Price'])\n",
        "df.to_csv('amazon_products.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_zpTYr_xZuxi"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"amazon_products.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHuKT-5De_Ig"
      },
      "source": [
        "\n",
        "**single item scraping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdjUF1pMmnAm"
      },
      "source": [
        "We will also try scraping a single products attributes which would also be useful to different businesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G054DoYrfmf2",
        "outputId": "159267fb-5dce-4644-facd-50c0aced3432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "     NEODRIFT 'CrystalTech' Silver Car Body Cover for Mitsubishi Lexus 450 (100% Water Resistant, Tailored Fit, All-Weather Protection, Multi-Layered & Breathable Fabric)\n",
            "    \n",
            "404900.0\n"
          ]
        }
      ],
      "source": [
        "# Connect to Website and pull in data\n",
        "\n",
        "URL = 'https://www.amazon.in/CrystalTech-Mitsubishi-Lexus-450-Multi-Layered/dp/B09ZHP54M3/ref=sr_1_1_sspa?crid=3KBXBTUB6P2KT&keywords=lexus&qid=1654459917&s=automotive&sprefix=lex%2Cautomotive%2C268&sr=1-1-spons&vehicle=Lexus%3ALX&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExMUlVMlIyVlBIVzhXJmVuY3J5cHRlZElkPUEwNTM3MjQ0MUozVk40Rk9PUDRRSCZlbmNyeXB0ZWRBZElkPUExMDA5MTk1M040WkhHTFBWMEtHSiZ3aWRnZXROYW1lPXNwX2F0ZiZhY3Rpb249Y2xpY2tSZWRpcmVjdCZkb05vdExvZ0NsaWNrPXRydWU='\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
        "\n",
        "title = soup2.find(id='productTitle').get_text()\n",
        "\n",
        "#price = soup2.find('span', attrs={'class':'a-offscreen'})\n",
        "price = float(soup2.find('span', attrs={'class':'a-offscreen'}).get_text().replace('.', '').replace('₹', '').replace(',', '').strip())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(title)\n",
        "print(price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zToBIDHjvlD",
        "outputId": "3011cd5f-29dd-4a57-d0ad-49f005c3505a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-06-05\n"
          ]
        }
      ],
      "source": [
        "# Create a Timestamp for your output to track when data was collected\n",
        "\n",
        "import datetime\n",
        "\n",
        "today = datetime.date.today()\n",
        "\n",
        "print(today)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vIKgZthrjvYT"
      },
      "outputs": [],
      "source": [
        "# Create CSV and write headers and data into the file\n",
        "\n",
        "import csv \n",
        "\n",
        "header = ['Title', 'Price', 'Date']\n",
        "data = [title, price, today]\n",
        "\n",
        "\n",
        "with open('AmazonWebScraperDataset.csv', 'w', newline='', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerow(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C0qgK9bj2zS",
        "outputId": "258203e7-7dd4-45c8-cc66-85244d499b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               Title     Price        Date\n",
            "0  \\n     NEODRIFT 'CrystalTech' Silver Car Body ...  404900.0  2022-06-05\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r'AmazonWebScraperDataset.csv')\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mSmggr0lvtD"
      },
      "source": [
        " creating  function by combining the code so that we canrun it to checkamazon prices every once in a while.this will then be used tocreate email alerts so that i can know when i can afford that cover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "VWVPuBG4j2Ml"
      },
      "outputs": [],
      "source": [
        "# Runs check_price after a set time and inputs data into your CSV\n",
        "def check_price():\n",
        "    URL = 'https://www.amazon.in/CrystalTech-Mitsubishi-Lexus-450-Multi-Layered/dp/B09ZHP54M3/ref=sr_1_1_sspa?crid=3KBXBTUB6P2KT&keywords=lexus&qid=1654459917&s=automotive&sprefix=lex%2Cautomotive%2C268&sr=1-1-spons&vehicle=Lexus%3ALX&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExMUlVMlIyVlBIVzhXJmVuY3J5cHRlZElkPUEwNTM3MjQ0MUozVk40Rk9PUDRRSCZlbmNyeXB0ZWRBZElkPUExMDA5MTk1M040WkhHTFBWMEtHSiZ3aWRnZXROYW1lPXNwX2F0ZiZhY3Rpb249Y2xpY2tSZWRpcmVjdCZkb05vdExvZ0NsaWNrPXRydWU='\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    page = requests.get(URL, headers=headers)\n",
        "\n",
        "    soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
        "\n",
        "    title = soup2.find(id='productTitle').get_text()\n",
        "    \n",
        "\n",
        "    #price = soup2.find('span', attrs={'class':'a-offscreen'})\n",
        "    price = float(soup2.find('span', attrs={'class':'a-offscreen'}).get_text().replace('.', '').replace('₹', '').replace(',', '').strip())\n",
        "    \n",
        "\n",
        "    import datetime\n",
        "\n",
        "    today = datetime.date.today()\n",
        "    \n",
        "    import csv \n",
        "\n",
        "    header = ['Title', 'Price', 'Date']\n",
        "    data = [title, price, today]\n",
        "\n",
        "    with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(data)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1qs0qyljvLn"
      },
      "outputs": [],
      "source": [
        "# Runs check_price after a set time and inputs data into your CSV\n",
        "\n",
        "while(True):\n",
        "    check_price()\n",
        "    time.sleep(86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2SECuU6ju8q"
      },
      "outputs": [],
      "source": [
        "# If uou want to try sending yourself an email (just for fun) when a price hits below a certain level you can try it\n",
        "# out with this script\n",
        "\n",
        "def send_mail():\n",
        "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
        "    server.ehlo()\n",
        "    #server.starttls()\n",
        "    server.ehlo()\n",
        "    server.login('mukamijeniffer6@gmail.com','xxxxxxxxxxxxxx')\n",
        "    \n",
        "    subject = \"Jenny the car cover you want is now available!\"\n",
        "    body = \"Jenifer, This is the moment we have been waiting for. Now is your chance to pick up the car cover of your dreams. Don't mess it up! Link here: https://www.amazon.in/CrystalTech-Mitsubishi-Lexus-450-Multi-Layered/dp/B09ZHP54M3/ref=sr_1_1_sspa?crid=3KBXBTUB6P2KT&keywords=lexus&qid=1654459917&s=automotive&sprefix=lex%2Cautomotive%2C268&sr=1-1-spons&vehicle=Lexus%3ALX&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExMUlVMlIyVlBIVzhXJmVuY3J5cHRlZElkPUEwNTM3MjQ0MUozVk40Rk9PUDRRSCZlbmNyeXB0ZWRBZElkPUExMDA5MTk1M040WkhHTFBWMEtHSiZ3aWRnZXROYW1lPXNwX2F0ZiZhY3Rpb249Y2xpY2tSZWRpcmVjdCZkb05vdExvZ0NsaWNrPXRydWU=\"\n",
        "   \n",
        "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
        "    \n",
        "    server.sendmail(\n",
        "        'mukamijeniffer6@gmail.com',\n",
        "        msg\n",
        "     \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJzwYMmBerI4"
      },
      "outputs": [],
      "source": [
        "#  sending myself an email (just for fun) when a price hits below a certain level \n",
        "# out with this script\n",
        "\n",
        "def send_mail():\n",
        "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
        "    server.ehlo()\n",
        "    #server.starttls()\n",
        "    server.ehlo()\n",
        "    server.login('AlexTheAnalyst95@gmail.com','xxxxxxxxxxxxxx')\n",
        "    \n",
        "    subject = \"The Shirt you want is below $15! Now is your chance to buy!\"\n",
        "    body = \"Alex, This is the moment we have been waiting for. Now is your chance to pick up the shirt of your dreams. Don't mess it up! Link here: https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data+analyst+tshirt&qid=1626655184&sr=8-3\"\n",
        "   \n",
        "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
        "    \n",
        "    server.sendmail(\n",
        "        'mukamijeniffer6@gmail.com',\n",
        "        msg\n",
        "     \n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2WqDHjLnhL4"
      },
      "source": [
        "web scraping using Python is a skill you can use to extract the data into a useful form that can then be imported and used in various ways.\n",
        "\n",
        "Some of the practical applications of web scraping could be:\n",
        "\n",
        "Gathering resume of candidates with a specific skill,\n",
        "Extracting tweets from twitter with specific hashtags,\n",
        "Lead generation in marketing,\n",
        "Scraping product details and reviews from e-commerce websites.\n",
        "\n",
        "Apart from the above use-cases, web scraping is widely used in natural language processing for extracting text from the websites for training a deep learning model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "However it also has its own challenge forinstance longevity. Since the web developers keep updating their websites, you cannot certainly rely on one scraper for too long. Even though the modifications might be minor, but they still might create a hindrance for you while fetching the data.so aother preffered realistic approach would be to use Application Programming Interfaces (APIs) offered by various websites & platforms\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled6 (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
